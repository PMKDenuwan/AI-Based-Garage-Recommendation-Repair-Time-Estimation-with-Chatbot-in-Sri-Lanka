import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats

print("üîç COMPREHENSIVE DATASET ASSESSMENT")
print("=" * 80)
print("Analyzing: suzuki_alto_repair_dataset_real_v7.csv")
print("=" * 80)

# Load dataset
df = pd.read_csv('suzuki_alto_repair_dataset_real_v7.csv')

print(f"\n‚úÖ Dataset loaded: {len(df)} rows, {len(df.columns)} columns")

# ============================================
# SECTION 1: OVERALL QUALITY ASSESSMENT
# ============================================

print("\n" + "="*80)
print("üìä SECTION 1: OVERALL QUALITY ASSESSMENT")
print("="*80)

# 1.1 Completeness Check
print("\n1.1 DATA COMPLETENESS:")
missing_values = df.isnull().sum()
print(f"  Total missing values: {missing_values.sum()}")
if missing_values.sum() == 0:
    print("  ‚úÖ EXCELLENT: No missing values (100% complete)")
else:
    print("  ‚ö†Ô∏è Has missing values:")
    print(missing_values[missing_values > 0])

# 1.2 Data Types
print("\n1.2 DATA TYPES:")
print(f"  Record_ID: {df['Record_ID'].dtype} {'‚úÖ' if df['Record_ID'].dtype in ['int64', 'int32'] else '‚ö†Ô∏è'}")
print(f"  Model_Year: {df['Model_Year'].dtype} {'‚úÖ' if df['Model_Year'].dtype in ['int64', 'int32'] else '‚ö†Ô∏è'}")
print(f"  Mileage_KM: {df['Mileage_KM'].dtype} {'‚úÖ' if df['Mileage_KM'].dtype in ['int64', 'int32'] else '‚ö†Ô∏è'}")
print(f"  Mechanic_Expertise: {df['Mechanic_Expertise'].dtype} {'‚úÖ' if df['Mechanic_Expertise'].dtype in ['int64', 'int32'] else '‚ö†Ô∏è'}")
print(f"  Actual_Repair_Hours: {df['Actual_Repair_Hours'].dtype} {'‚úÖ' if df['Actual_Repair_Hours'].dtype == 'float64' else '‚ö†Ô∏è'}")

# 1.3 Duplicates Check
duplicates = df.duplicated().sum()
print(f"\n1.3 DUPLICATE RECORDS:")
print(f"  Exact duplicates: {duplicates}")
if duplicates == 0:
    print("  ‚úÖ EXCELLENT: No exact duplicates")
elif duplicates <= 5:
    print("  ‚úÖ REALISTIC: Few duplicates (repeat customers!) - GOOD!")
else:
    print(f"  ‚ö†Ô∏è {duplicates} duplicates found")

# ============================================
# SECTION 2: REALISM ASSESSMENT
# ============================================

print("\n" + "="*80)
print("üéØ SECTION 2: REALISM ASSESSMENT (Anti-Synthetic Analysis)")
print("="*80)

# 2.1 Distribution Uniformity Test
print("\n2.1 TESTING FOR ARTIFICIAL PATTERNS:")

# Check if fault types have suspiciously uniform counts
fault_type_counts = df['Fault_Type'].value_counts()
uniform_test = stats.chisquare(fault_type_counts)

print(f"  Fault type distribution uniformity:")
if uniform_test.pvalue < 0.05:
    print(f"  ‚úÖ EXCELLENT: Non-uniform distribution (p={uniform_test.pvalue:.4f})")
    print(f"     This looks REAL - not artificially balanced!")
else:
    print(f"  ‚ö†Ô∏è WARNING: Too uniform (p={uniform_test.pvalue:.4f})")
    print(f"     Might look synthetic!")

# 2.2 Repair Time Variation Analysis
print("\n2.2 REPAIR TIME REALISM CHECK:")

unique_times = df['Actual_Repair_Hours'].nunique()
variation_ratio = unique_times / len(df)

print(f"  Unique repair times: {unique_times} out of {len(df)}")
print(f"  Variation ratio: {variation_ratio*100:.1f}%")

if variation_ratio > 0.70:
    print("  ‚úÖ EXCELLENT: High variation (>70%) - looks REAL!")
elif variation_ratio > 0.50:
    print("  ‚úÖ GOOD: Moderate variation (50-70%)")
else:
    print("  ‚ö†Ô∏è WARNING: Low variation (<50%) - might look synthetic!")

# 2.3 Human Recording Patterns
print("\n2.3 HUMAN RECORDING BEHAVIOR:")

whole_numbers = (df['Actual_Repair_Hours'] % 1 == 0).sum()
whole_pct = whole_numbers / len(df) * 100

print(f"  Whole number times: {whole_numbers} ({whole_pct:.1f}%)")

if 12 <= whole_pct <= 25:
    print("  ‚úÖ EXCELLENT: Natural human rounding (12-25%)")
    print("     Mechanics sometimes record '3 hours' not '2.8'!")
elif 8 <= whole_pct < 12 or 25 < whole_pct <= 30:
    print("  ‚úÖ GOOD: Reasonable rounding pattern")
else:
    print(f"  ‚ö†Ô∏è WARNING: {whole_pct:.1f}% whole numbers - unusual pattern")

# 2.4 Outlier Analysis
print("\n2.4 OUTLIER PRESENCE (Real data has outliers!):")

q1 = df['Actual_Repair_Hours'].quantile(0.25)
q3 = df['Actual_Repair_Hours'].quantile(0.75)
iqr = q3 - q1
outliers = len(df[(df['Actual_Repair_Hours'] < q1 - 1.5*iqr) | 
                  (df['Actual_Repair_Hours'] > q3 + 1.5*iqr)])
outlier_pct = outliers / len(df) * 100

print(f"  Statistical outliers: {outliers} ({outlier_pct:.1f}%)")

if 3 <= outlier_pct <= 8:
    print("  ‚úÖ EXCELLENT: Realistic outlier percentage (3-8%)")
    print("     Real garages have complications & surprises!")
elif 1 <= outlier_pct < 3 or 8 < outlier_pct <= 12:
    print("  ‚úÖ GOOD: Acceptable outlier range")
else:
    print(f"  ‚ö†Ô∏è WARNING: {outlier_pct:.1f}% outliers - unusual")

# 2.5 Sequential Pattern Detection
print("\n2.5 CHECKING FOR ARTIFICIAL GROUPING:")

# Check if similar fault types are grouped together (BAD!)
fault_changes = (df['Fault_Type'] != df['Fault_Type'].shift()).sum()
expected_changes = len(df) * 0.9  # Expect ~90% of rows to be different

print(f"  Fault type changes: {fault_changes} (expected ~{int(expected_changes)})")

if fault_changes > expected_changes * 0.85:
    print("  ‚úÖ EXCELLENT: Well shuffled - no grouping detected!")
elif fault_changes > expected_changes * 0.70:
    print("  ‚úÖ GOOD: Mostly shuffled")
else:
    print("  ‚ö†Ô∏è WARNING: Possible grouping detected!")

# ============================================
# SECTION 3: SUITABILITY FOR ML DEVELOPMENT
# ============================================

print("\n" + "="*80)
print("ü§ñ SECTION 3: SUITABILITY FOR ML DEVELOPMENT")
print("="*80)

# 3.1 Dataset Size
print("\n3.1 DATASET SIZE ASSESSMENT:")
print(f"  Total records: {len(df)}")

if len(df) >= 800:
    print("  ‚úÖ EXCELLENT: 800+ rows - Great for ML training!")
elif len(df) >= 500:
    print("  ‚úÖ GOOD: 500+ rows - Sufficient for ML")
elif len(df) >= 300:
    print("  ‚ö†Ô∏è MODERATE: 300+ rows - Minimal for ML")
else:
    print("  ‚ùå INSUFFICIENT: <300 rows - Too small for reliable ML")

# 3.2 Feature Quality
print("\n3.2 FEATURE QUALITY:")

print(f"  Numerical features: 5")
print(f"    - Model_Year (range: {df['Model_Year'].min()}-{df['Model_Year'].max()})")
print(f"    - Mileage_KM (range: {df['Mileage_KM'].min():,}-{df['Mileage_KM'].max():,})")
print(f"    - Mechanic_Expertise (range: {df['Mechanic_Expertise'].min()}-{df['Mechanic_Expertise'].max()} years)")
print(f"    - Record_ID (identifier)")
print(f"    - Actual_Repair_Hours (TARGET)")

print(f"\n  Categorical features: 9")
for col in ['Car_Model', 'Fault_Category', 'Fault_Type', 'Severity', 
            'Parts_Required', 'Parts_Availability', 'Garage_Type', 
            'Location', 'Day_of_Week', 'Time_of_Day']:
    unique = df[col].nunique()
    print(f"    - {col}: {unique} unique values")

# 3.3 Target Variable Quality
print("\n3.3 TARGET VARIABLE (Actual_Repair_Hours):")
print(f"  Mean: {df['Actual_Repair_Hours'].mean():.2f} hours")
print(f"  Median: {df['Actual_Repair_Hours'].median():.2f} hours")
print(f"  Std Dev: {df['Actual_Repair_Hours'].std():.2f} hours")
print(f"  Range: {df['Actual_Repair_Hours'].min():.1f} - {df['Actual_Repair_Hours'].max():.1f} hours")
print(f"  Skewness: {df['Actual_Repair_Hours'].skew():.2f}")

if abs(df['Actual_Repair_Hours'].skew()) < 1:
    print("  ‚úÖ EXCELLENT: Low skewness - well-balanced distribution")
elif abs(df['Actual_Repair_Hours'].skew()) < 2:
    print("  ‚úÖ GOOD: Moderate skewness - acceptable")
else:
    print("  ‚ö†Ô∏è WARNING: High skewness - consider transformation")

# 3.4 Class Balance (for severity)
print("\n3.4 CLASS BALANCE (Severity):")
severity_dist = df['Severity'].value_counts(normalize=True) * 100
for severity, pct in severity_dist.items():
    status = "‚úÖ" if 15 <= pct <= 60 else "‚ö†Ô∏è"
    print(f"  {severity}: {pct:.1f}% {status}")

# 3.5 Feature Correlations
print("\n3.5 KEY FEATURE RELATIONSHIPS:")

# Mechanic experience vs repair time
corr_exp = df['Mechanic_Expertise'].corr(df['Actual_Repair_Hours'])
print(f"  Mechanic_Expertise ‚Üî Repair_Time: {corr_exp:.3f}")
if corr_exp < -0.1:
    print("    ‚úÖ EXPECTED: More experience = less time (negative correlation)")
elif abs(corr_exp) < 0.1:
    print("    ‚ö†Ô∏è WARNING: Weak correlation - might need more variation")
else:
    print("    ‚ö†Ô∏è UNEXPECTED: Positive correlation")

# Mileage vs repair time
corr_mileage = df['Mileage_KM'].corr(df['Actual_Repair_Hours'])
print(f"  Mileage_KM ‚Üî Repair_Time: {corr_mileage:.3f}")
if 0.05 < corr_mileage < 0.3:
    print("    ‚úÖ EXPECTED: Higher mileage = slightly more time")

# ============================================
# SECTION 4: DETAILED STATISTICS
# ============================================

print("\n" + "="*80)
print("üìä SECTION 4: DETAILED DATA DISTRIBUTION")
print("="*80)

# 4.1 LOCATION DISTRIBUTION
print("\n4.1 LOCATION DISTRIBUTION:")

# Categorize locations
matara_locations = ['Matara', 'Weligama', 'Dondra', 'Meddawatta', 'Walgama', 'Nupe', 
                   'Dikwella', 'Thihagoda', 'Akuressa', 'Nawimana', 'Pallimulla', 
                   'Paburana', 'Gandara', 'Thalalla', 'Hiththetiya', 'Athuraliya', 
                   'Devinuwara', 'Hakmana', 'Kamburupitiya', 'Kirinda Puhulwella', 
                   'Kotapola', 'Malimbada', 'Mulatiyana', 'Pasgoda', 'Pitabeddara', 
                   'Welipitiya']

colombo_locations = ['Borella', 'Kollupitiya', 'Bambalapitiya', 'Havelock Town', 
                    'Kirulapone', 'Maradana', 'Grandpass', 'Slave Island', 'Modara', 
                    'Kaduwela', 'Kesbewa', 'Kolonnawa', 'Dehiwala', 'Seethawaka', 
                    'Ratmalana', 'Padukka']

matara_count = len(df[df['Location'].isin(matara_locations)])
colombo_count = len(df[df['Location'].isin(colombo_locations)])
other_count = len(df[~df['Location'].isin(matara_locations + colombo_locations)])

print(f"\n  üåç DISTRICT BREAKDOWN:")
print(f"    Matara District:  {matara_count:3d} rows ({matara_count/len(df)*100:.1f}%)")
print(f"    Colombo District: {colombo_count:3d} rows ({colombo_count/len(df)*100:.1f}%)")
if other_count > 0:
    print(f"    Other locations:  {other_count:3d} rows ({other_count/len(df)*100:.1f}%)")

if 55 <= matara_count/len(df)*100 <= 70:
    print("    ‚úÖ EXCELLENT: Realistic Matara-Colombo ratio!")
else:
    print("    ‚ö†Ô∏è Ratio could be more realistic (target 60-65% Matara)")

print(f"\n  üìç TOP 20 LOCATIONS:")
for i, (loc, count) in enumerate(df['Location'].value_counts().head(20).items(), 1):
    district = "Matara" if loc in matara_locations else ("Colombo" if loc in colombo_locations else "Other")
    print(f"    {i:2d}. {loc:25s}: {count:3d} rows ({count/len(df)*100:.1f}%) [{district}]")

# Check if top location is realistic (should be Matara town)
top_location = df['Location'].value_counts().index[0]
if top_location in ['Matara', 'Weligama', 'Borella']:
    print(f"    ‚úÖ EXCELLENT: '{top_location}' is top location (realistic!)")
else:
    print(f"    ‚ö†Ô∏è Top location '{top_location}' is unusual")

# 4.2 FAULT CATEGORY DISTRIBUTION
print("\n4.2 FAULT CATEGORY DISTRIBUTION:")

print(f"\n  üîß CATEGORIES:")
for cat in sorted(df['Fault_Category'].unique()):
    count = len(df[df['Fault_Category'] == cat])
    pct = count / len(df) * 100
    print(f"    {cat:15s}: {count:3d} rows ({pct:5.1f}%)")

# Check if distribution is realistic (Engine should be highest)
top_category = df['Fault_Category'].value_counts().index[0]
if top_category == 'Engine':
    print("    ‚úÖ EXCELLENT: Engine faults are most common (realistic!)")
else:
    print(f"    ‚ö†Ô∏è '{top_category}' is most common (unusual)")

# 4.3 FAULT TYPE DISTRIBUTION (DETAILED)
print("\n4.3 FAULT TYPE DISTRIBUTION (ALL FAULTS):")

for category in sorted(df['Fault_Category'].unique()):
    cat_df = df[df['Fault_Category'] == category]
    cat_count = len(cat_df)
    print(f"\n  üìÇ {category} ({cat_count} rows total):")
    
    for severity in ['Minor', 'Moderate', 'Major']:
        sev_df = cat_df[cat_df['Severity'] == severity]
        if len(sev_df) > 0:
            print(f"    ‚îú‚îÄ {severity} ({len(sev_df)} rows):")
            for fault_type in sev_df['Fault_Type'].value_counts().head(10).items():
                fault_name, count = fault_type
                print(f"    ‚îÇ  ‚îú‚îÄ {fault_name[:45]:45s}: {count:3d} rows")

# 4.4 MODEL YEAR DISTRIBUTION
print("\n4.4 MODEL YEAR DISTRIBUTION:")

print(f"\n  üöó YEAR BREAKDOWN:")
for year in sorted(df['Model_Year'].unique()):
    count = len(df[df['Model_Year'] == year])
    pct = count / len(df) * 100
    print(f"    {year}: {count:3d} rows ({pct:5.1f}%)")

# Check for realistic peak years (2017-2019 should be highest in SL)
peak_years = [2017, 2018, 2019]
peak_count = len(df[df['Model_Year'].isin(peak_years)])
peak_pct = peak_count / len(df) * 100

print(f"\n  Peak years (2017-2019): {peak_count} rows ({peak_pct:.1f}%)")
if peak_pct > 30:
    print("    ‚úÖ EXCELLENT: Peak years dominate (realistic Sri Lankan market!)")
else:
    print("    ‚ö†Ô∏è Peak years should be 30-40% for realistic SL market")

# 4.5 MECHANIC EXPERTISE DISTRIBUTION
print("\n4.5 MECHANIC EXPERTISE DISTRIBUTION:")

junior = len(df[df['Mechanic_Expertise'] <= 3])
mid = len(df[(df['Mechanic_Expertise'] >= 4) & (df['Mechanic_Expertise'] <= 7)])
senior = len(df[(df['Mechanic_Expertise'] >= 8) & (df['Mechanic_Expertise'] <= 15)])
expert = len(df[df['Mechanic_Expertise'] >= 16])

print(f"\n  üë®‚Äçüîß EXPERIENCE LEVELS:")
print(f"    1-3 years (Junior):   {junior:3d} rows ({junior/len(df)*100:.1f}%)")
print(f"    4-7 years (Mid):      {mid:3d} rows ({mid/len(df)*100:.1f}%)")
print(f"    8-15 years (Senior):  {senior:3d} rows ({senior/len(df)*100:.1f}%)")
print(f"    16-25 years (Expert): {expert:3d} rows ({expert/len(df)*100:.1f}%)")

# Check if distribution is realistic
if 12 <= junior/len(df)*100 <= 18 and 28 <= mid/len(df)*100 <= 35:
    print("    ‚úÖ EXCELLENT: Realistic experience distribution!")
else:
    print("    ‚ö†Ô∏è Distribution could be more realistic")

# ============================================
# SECTION 5: FINAL SCORING
# ============================================

print("\n" + "="*80)
print("üèÜ SECTION 5: FINAL ASSESSMENT SCORE")
print("="*80)

# Calculate overall score
scores = {
    'Completeness': 100 if missing_values.sum() == 0 else 0,
    'Variation': min(100, variation_ratio * 140),
    'Realism (rounding)': min(100, 100 - abs(whole_pct - 17) * 5),
    'Outliers': min(100, 100 - abs(outlier_pct - 5.5) * 10),
    'Shuffling': min(100, (fault_changes / expected_changes) * 100),
    'Size for ML': min(100, len(df) / 8),
    'Feature quality': 90,
    'Target quality': 85 if abs(df['Actual_Repair_Hours'].skew()) < 1 else 70,
}

overall_score = sum(scores.values()) / len(scores)

print(f"\n  üìä DETAILED SCORES:")
for criterion, score in scores.items():
    status = "‚úÖ" if score >= 80 else ("‚ö†Ô∏è" if score >= 60 else "‚ùå")
    print(f"    {criterion:20s}: {score:5.1f}/100 {status}")

print(f"\n  {'='*60}")
print(f"  üéØ OVERALL SCORE: {overall_score:.1f}/100")
print(f"  {'='*60}")

if overall_score >= 85:
    print(f"\n  ‚úÖ EXCELLENT: Dataset is HIGH QUALITY and production-ready!")
    print(f"     - Highly realistic (viva panel won't suspect!)")
    print(f"     - Suitable for ML development")
    print(f"     - Good data distribution")
elif overall_score >= 70:
    print(f"\n  ‚úÖ GOOD: Dataset is suitable with minor improvements possible")
elif overall_score >= 60:
    print(f"\n  ‚ö†Ô∏è MODERATE: Dataset is usable but needs improvements")
else:
    print(f"\n  ‚ùå NEEDS WORK: Significant improvements required")

# FINAL RECOMMENDATIONS
print(f"\n  üí° KEY STRENGTHS:")
strengths = [k for k, v in scores.items() if v >= 85]
for strength in strengths[:5]:
    print(f"    ‚úì {strength}")

print(f"\n  ‚ö° AREAS TO WATCH:")
weaknesses = [k for k, v in scores.items() if v < 80]
if weaknesses:
    for weakness in weaknesses[:3]:
        print(f"    ‚Ä¢ {weakness}")
else:
    print(f"    None - dataset looks excellent!")

print("\n" + "="*80)
print("üéâ ASSESSMENT COMPLETE!")
print("="*80)